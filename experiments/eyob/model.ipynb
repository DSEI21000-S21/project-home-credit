{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T18:51:47.942058Z",
     "start_time": "2021-05-10T18:51:37.868985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/eyobmanhardt/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# sklearn utilties \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# classifiers  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sampling \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# useful libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T18:54:03.767501Z",
     "start_time": "2021-05-10T18:51:47.992046Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import dropbox\n",
    "#import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "from utils.data_extract_utils import extract_zip, extract_features_from_bureau\n",
    "\n",
    "# say someting ....\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "# Connect to dropbox\n",
    "dbx = dropbox.Dropbox('cHV7yAR0J6YAAAAAAAAAAVQ1NLCrOwerbaNltPWHslYXKuUTJ5_wfgJsuFcmx83o')\n",
    "\n",
    "data = {}\n",
    "for entry in dbx.files_list_folder('').entries:\n",
    "    response = dbx.files_download('/{}'.format(entry.name))\n",
    "    \n",
    "    if 'zip' in entry.name:\n",
    "        content = extract_zip(response[1].content)\n",
    "    \n",
    "        for file in content:\n",
    "            df = pd.read_csv(file[1])\n",
    "            data[entry.name.replace('.csv.zip', '')] = df\n",
    "            \n",
    "import gc\n",
    "\n",
    "\n",
    "def get_temp_data(data):\n",
    "    ## combine train and test set\n",
    "    train_test = data['application_train'].append(data['application_test']).reset_index()\n",
    "\n",
    "    # join tables: bureau, bureau_balance - 1 min to execute\n",
    "    bb = extract_features_from_bureau(data['bureau'], data['bureau_balance'])\n",
    "    \n",
    "    # Join ID same datatype\n",
    "    train_test['SK_ID_CURR'] = train_test['SK_ID_CURR'].astype('Int64')\n",
    "    bb.index = bb.index.astype('Int64')\n",
    "\n",
    "    # Join Bureau(s) and Application tables\n",
    "    df = train_test.join(bb, how='left', on='SK_ID_CURR', lsuffix='_left', rsuffix='_right')\n",
    "    \n",
    "    # drop index column generated by groupby\n",
    "    df.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    # clean memory \n",
    "    del bb\n",
    "    gc.collect()\n",
    "    \n",
    "    # preprocess \n",
    "    ## DAYS_EMPLOYED\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    df['DAYS_EMPLOYED'] =df['DAYS_EMPLOYED'].apply(lambda x: abs(x))\n",
    "\n",
    "    df['BIRTH_IN_YEARS'] = df['DAYS_BIRTH'].apply(lambda x: abs(x)/365)\n",
    "\n",
    "    # remove 4 instances\n",
    "    df = df.loc[df['CODE_GENDER'] != 'XNA', :]\n",
    "\n",
    "\n",
    "    # add new features \n",
    "    df['ALL_EXT_SOURCE_MEAN']=df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY']/df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL']/df['CNT_FAM_MEMBERS']\n",
    "    df['INCOME_CREDIT_IN_PERCENTAGE '] = df['AMT_INCOME_TOTAL']/df['AMT_CREDIT']\n",
    "    df['ANNUITY_INCOME_IN_PERCENTAGE'] = df['AMT_ANNUITY']/df['AMT_INCOME_TOTAL']\n",
    "\n",
    "    ## label encoder for binary values\n",
    "    bin_features = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
    "\n",
    "    replace_dict = {'CODE_GENDER': {'M': 0, 'F': 1},\n",
    "                    'FLAG_OWN_CAR': {'Y': 0, 'N': 1}, \n",
    "                    'FLAG_OWN_REALTY':{'Y': 0, 'N': 1} }\n",
    "    df.replace(replace_dict, inplace=True)\n",
    "    \n",
    "    # fix inf values\n",
    "    df['AMT_CREDIT_DEBT_RATIO'] = df['AMT_CREDIT_DEBT_RATIO'].apply(lambda x: x if ~np.isinf(x) else 0)\n",
    "    \n",
    "    # test ids\n",
    "    test_ids = df.loc[df['TARGET'].isnull(), 'SK_ID_CURR']\n",
    "    \n",
    "    # drop ids\n",
    "    df.drop(columns=['SK_ID_CURR'], axis=1, inplace=True)\n",
    "    \n",
    "    return df, test_ids\n",
    "\n",
    "## Cell takes 2m 30 sec to execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T18:56:03.364413Z",
     "start_time": "2021-05-10T18:54:11.946105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 116)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, test_ids = get_temp_data(data)\n",
    "\n",
    "X, y = (df.loc[df['TARGET'].notnull(), :].drop(columns=['TARGET'], axis=1), \n",
    "        df.loc[df['TARGET'].notnull(), 'TARGET'])\n",
    "\n",
    "# test set for kaggle\n",
    "X_test = df.loc[df['TARGET'].isnull(), :].drop(columns=['TARGET'], axis=1)\n",
    "\n",
    "\n",
    "# df['AMT_CREDIT_DEBT_RATIO'] = df['AMT_CREDIT_DEBT_RATIO'].apply(lambda x: x if ~np.isinf(x) else 0)\n",
    "# df.drop(columns=['SK_ID_CURR'], axis=1, inplace=True)\n",
    "\n",
    "# train_df = df.loc[df['TARGET'].notnull(), :]\n",
    "# test_df = df.loc[df['TARGET'].isnull(), :]\n",
    "\n",
    "# y = train_df['TARGET']\n",
    "\n",
    "# train_df.drop(columns= ['TARGET'], axis=1, inplace=True)\n",
    "# test_df.drop(columns= ['TARGET'], axis=1, inplace=True)\n",
    "\n",
    "# X = train_df\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.2, random_state= 42, stratify=y)\n",
    "\n",
    "# columns for pipeline\n",
    "cat_cols = list(X.select_dtypes(include=object).columns)\n",
    "num_cols = list(X.select_dtypes(include=[int, float]).columns)\n",
    "len(cat_cols),len(num_cols)\n",
    "\n",
    "## Cell takes 1m 59 sec to execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T02:39:08.590179Z",
     "start_time": "2021-05-07T02:39:08.581480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48744,), (48744, 129))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check \n",
    "test_ids.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T18:56:09.205477Z",
     "start_time": "2021-05-10T18:56:09.196306Z"
    }
   },
   "outputs": [],
   "source": [
    "## pipelines\n",
    "\n",
    "# one hot encoding \n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    ('cat_imp', SimpleImputer(strategy='most_frequent', add_indicator=False)),\n",
    "  ('one_hot_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# numerical \n",
    "numerical_pipe = Pipeline(steps=[\n",
    "    ('num_imp', SimpleImputer(strategy='median', add_indicator=False)),\n",
    "    (\"scale\", StandardScaler())  \n",
    "])\n",
    "\n",
    "\n",
    "# transform columns \n",
    "column_transformer = ColumnTransformer(transformers=[    \n",
    "    ('num_pip', numerical_pipe, num_cols),\n",
    "    ('cat_pipe', categorical_pipe, cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T18:56:27.100865Z",
     "start_time": "2021-05-10T18:56:13.268554Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply preprocessing to X\n",
    "X_trans = column_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T08:23:15.737811Z",
     "start_time": "2021-05-10T08:23:15.728923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before transformations shape: (307507, 129)\n",
      "after transformations shape: (307507, 249)\n"
     ]
    }
   ],
   "source": [
    "print('before transformations shape:', X.shape)\n",
    "print('after transformations shape:', X_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base-case no sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T08:25:43.798464Z",
     "start_time": "2021-05-10T08:24:04.584025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifaction report on training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    226145\n",
      "           1       1.00      1.00      1.00     19860\n",
      "\n",
      "    accuracy                           1.00    246005\n",
      "   macro avg       1.00      1.00      1.00    246005\n",
      "weighted avg       1.00      1.00      1.00    246005\n",
      "\n",
      "--------------------------------------------------------------\n",
      "classifaction report on validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     56537\n",
      "           1       0.59      0.00      0.01      4965\n",
      "\n",
      "    accuracy                           0.92     61502\n",
      "   macro avg       0.76      0.50      0.48     61502\n",
      "weighted avg       0.89      0.92      0.88     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_trans, y, test_size=0.2, random_state= 42, stratify=y)\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_val = rf.predict(X_val)\n",
    "print('classifaction report on training set')\n",
    "print(classification_report(y_train, y_pred_train, labels=[0,1]))\n",
    "print('--------------------------------------------------------------')\n",
    "print('classifaction report on validation set')\n",
    "print(classification_report(y_val, y_pred_val, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the model above is garbage - since the the the dummy classifier (i.e i==0) will score 96% accuracy; therefore the notion accuracy in this sense is meaningless. Our objective is to increase the f1-score for both classes. Ideally we want the f1-score for both classes to be close to 1 then we can use the accuracy score evaluate the performance of the model. We're going to attempt different sampling techniques to achieve our goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# under sampling - majority class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T03:00:40.914213Z",
     "start_time": "2021-05-07T02:59:17.835740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy 0.6825066176361723 +/- 0.002928926654912159\n",
      "Balanced accuracy 0.6871553626682388 +/- 0.001388504080283691\n"
     ]
    }
   ],
   "source": [
    "# Split data \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trans, y, test_size=0.2, random_state= 42, stratify=y)\n",
    "\n",
    "# perform cross validation on entire data \n",
    "model_under = make_pipeline(\n",
    "    RandomUnderSampler(random_state=42),\n",
    "    LogisticRegression(solver='lbfgs', random_state=42)\n",
    ")\n",
    "\n",
    "cv_results_under = cross_validate(\n",
    "     model_under, X_trans, y, scoring=\"balanced_accuracy\",\n",
    "     return_train_score=True, return_estimator=True,\n",
    "     n_jobs=-1\n",
    " )\n",
    "\n",
    "# average cross validation score \n",
    "print(\"Balanced accuracy {} +/- {}\".format(cv_results_under['test_score'].mean(), \n",
    "                                           cv_results_under['test_score'].std()))\n",
    "\n",
    "scores = []\n",
    "for fold_id, cv_model in enumerate(cv_results_under[\"estimator\"]):\n",
    "     scores.append(balanced_accuracy_score(y_val, cv_model.predict(X_val)))\n",
    "\n",
    "# average score on each k fold model - on left out set: X_val\n",
    "# this is to confirm avg cross validation score is not too optimistic \n",
    "# i.e this should be very similar to our first score \n",
    "print(\"Balanced accuracy {} +/- {}\".format(np.mean(scores), \n",
    "                                           np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# over-sampling the minority class - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-10T16:48:23.674Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell will overload your computer .. at least that's the case for me \n",
    "\n",
    "# Split data \n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_trans, y, test_size=0.2, random_state= 42, stratify=y)\n",
    "\n",
    "# # perform cross validation on entire data \n",
    "# model_smote = make_pipeline(\n",
    "#     SMOTE(sampling_strategy='minority'),\n",
    "#     LogisticRegression(solver='lbfgs', random_state=42)\n",
    "# )\n",
    "\n",
    "# cv_results_smote = cross_validate(\n",
    "#      model_smote, X_trans, y, scoring=\"balanced_accuracy\",\n",
    "#      return_train_score=True, return_estimator=True,\n",
    "#     cv= 3,\n",
    "#      n_jobs=-1\n",
    "#  )\n",
    "\n",
    "# # average cross validation score \n",
    "# print(\"Balanced accuracy {} +/- {}\".format(cv_results_smote['test_score'].mean(), \n",
    "#                                            cv_results_smote['test_score'].std()))\n",
    "\n",
    "# scores = []\n",
    "# for fold_id, cv_model in enumerate(cv_results_smote[\"estimator\"]):\n",
    "#      scores.append(balanced_accuracy_score(y_val, cv_model.predict(X_val)))\n",
    "\n",
    "# # average score on each k fold model - on left out set: X_val\n",
    "# # this is to confirm avg cross validation score is not too optimistic \n",
    "# # i.e this should be very similar to our first score \n",
    "# print(\"Balanced accuracy {} +/- {}\".format(np.mean(scores), \n",
    "#                                            np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE*** Over-sampling the minority class using synthtic data (i.e SMOTE) is going to increase the number of instances in our data frame. Therefore, calculating the cross-validation score where we put the sampling method alongside the ml model in a single pipeline (what we did above) is going to be unrealistic to compute with our new data set. At least for my computer (EYOB), it breaks my kernal. We can still use cross validation method - to provide a performance score for this sampling technique; however, our methodology will change slightly. Specfically, we will sample the data using smote sampling then we will manually split the data in k folds. The splitting needs to be done manually - so the model can be executed with reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T20:47:56.973229Z",
     "start_time": "2021-05-10T20:41:35.877201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7057111412456027\n"
     ]
    }
   ],
   "source": [
    "# smote sampling\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_resample(X_trans,y)\n",
    "\n",
    "# split data into k folds\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "score_val = []\n",
    "for train_index, val_index in sss.split(X_sm, y_sm):\n",
    "    # initalize model \n",
    "    model = LogisticRegression(solver='lbfgs', random_state=42,max_iter=1000)\n",
    "    # fit model \n",
    "    model.fit(X_sm[train_index], y_sm[train_index])\n",
    "    # test model\n",
    "    score_val.append(balanced_accuracy_score(y_sm[val_index], model.predict(X_sm[val_index])))\n",
    "## takes about 6 min to execute ... 2 min per fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T21:28:01.325053Z",
     "start_time": "2021-05-10T21:28:01.320469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7057111412456027 +/- 0.000725206404900559\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score_val), '+/-', np.std(score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:10:54.475919Z",
     "start_time": "2021-05-10T19:08:24.692988Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifaction report on training set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70    226145\n",
      "           1       0.70      0.71      0.71    226146\n",
      "\n",
      "    accuracy                           0.71    452291\n",
      "   macro avg       0.71      0.71      0.71    452291\n",
      "weighted avg       0.71      0.71      0.71    452291\n",
      "\n",
      "--------------------------------------------------------------\n",
      "classifaction report on validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71     56537\n",
      "           1       0.71      0.71      0.71     56536\n",
      "\n",
      "    accuracy                           0.71    113073\n",
      "   macro avg       0.71      0.71      0.71    113073\n",
      "weighted avg       0.71      0.71      0.71    113073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_sm, y_sm, test_size=0.2, random_state= 42, stratify=y_sm)\n",
    "logreg= LogisticRegression(solver='lbfgs', random_state=42,max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "y_pred_val = logreg.predict(X_val)\n",
    "print('classifaction report on training set')\n",
    "print(classification_report(y_train, y_pred_train, labels=[0,1]))\n",
    "print('--------------------------------------------------------------')\n",
    "print('classifaction report on validation set')\n",
    "print(classification_report(y_val, y_pred_val, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:16:52.065629Z",
     "start_time": "2021-05-10T19:16:51.386892Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_trans = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:17:12.892378Z",
     "start_time": "2021-05-10T19:17:12.233537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307511</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.482375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307512</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.803179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307513</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.338258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307514</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.240855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307515</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.678379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR    TARGET\n",
       "307511     100001  0.482375\n",
       "307512     100005  0.803179\n",
       "307513     100013  0.338258\n",
       "307514     100028  0.240855\n",
       "307515     100038  0.678379"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_test_pred = logreg.predict_proba(X_test_trans )[:,1]\n",
    "\n",
    "submission5_dict = {'SK_ID_CURR': test_ids, \n",
    "            'TARGET': logreg_test_pred }\n",
    "logreg_submission5 = pd.DataFrame(submission5_dict)\n",
    "logreg_submission5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T19:18:04.485611Z",
     "start_time": "2021-05-10T19:18:03.886614Z"
    }
   },
   "outputs": [],
   "source": [
    "#logreg_submission5.to_csv('submission5.csv', index=False) ## kaggle score 74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE sampling provides the best score without tuning the parameters. The actual test set (unseen data) performed better than the validation set; however, not by too much. More importantly, this model is providing us with scores that's is algined with the validations performance(i.e over-fitting does not occur). Thus, we can consider the SMOTE model performance as our baseline and go from here. ---> SMOTE is going to be the sampling technique we use from now on!!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next step: Selecting the best features\n",
    "\n",
    "How do we select the best feature for our ML model\n",
    "\n",
    "***Brainstorm - notes from class***\n",
    "\n",
    "Feature selections methods:\n",
    "- Percent missing value \n",
    "    - remove features with high percent of missing values\n",
    "- Amount of variation\n",
    "    - remove feature that don't vary in values\n",
    "- Pairwise-correlations \n",
    "    - drop features that correlate with another (only drop one)\n",
    "- multicollinearity\n",
    "- correlation with the target\n",
    "- cluster analysis\n",
    "- PCA\n",
    "- forward/backward/stepwise selection \n",
    "- Lasso - drops coeif value to 0\n",
    "- Tree based models \n",
    "    - feature importance \n",
    "    \n",
    "***How do we check the feature we choose impacts our model***\n",
    "- need to check whether or not the specific feature makes any contribution to the model good/bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application = data['application_train'].append(data['application_test']).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features from application dataset \n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model evaluation procedure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
